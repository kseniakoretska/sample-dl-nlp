{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "897195b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5        0.         0.         0.         0.         0.\n",
      "  0.5        0.5        0.         0.         0.5        0.\n",
      "  0.         0.        ]\n",
      " [0.         0.40824829 0.40824829 0.         0.40824829 0.\n",
      "  0.         0.         0.40824829 0.         0.         0.40824829\n",
      "  0.40824829 0.        ]\n",
      " [0.         0.40824829 0.40824829 0.         0.40824829 0.\n",
      "  0.         0.         0.40824829 0.         0.         0.40824829\n",
      "  0.40824829 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.40824829 0.40824829 0.         0.40824829 0.\n",
      "  0.         0.         0.40824829 0.         0.         0.40824829\n",
      "  0.40824829 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.5        0.         0.         0.         0.         0.\n",
      "  0.5        0.5        0.         0.         0.5        0.\n",
      "  0.         0.        ]\n",
      " [0.5        0.         0.         0.         0.         0.\n",
      "  0.5        0.5        0.         0.         0.5        0.\n",
      "  0.         0.        ]\n",
      " [0.         0.40824829 0.40824829 0.         0.40824829 0.\n",
      "  0.         0.         0.40824829 0.         0.         0.40824829\n",
      "  0.40824829 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.5        0.         0.         0.         0.         0.\n",
      "  0.5        0.5        0.         0.         0.5        0.\n",
      "  0.         0.        ]\n",
      " [0.         0.40824829 0.40824829 0.         0.40824829 0.\n",
      "  0.         0.         0.40824829 0.         0.         0.40824829\n",
      "  0.40824829 0.        ]\n",
      " [0.         0.40824829 0.40824829 0.         0.40824829 0.\n",
      "  0.         0.         0.40824829 0.         0.         0.40824829\n",
      "  0.40824829 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def ppmi_weighting(docs):\n",
    "    # Tokenize the documents and count co-occurrences\n",
    "    vectorizer = CountVectorizer(tokenizer=lambda text: text.split(), binary=True)\n",
    "    X = vectorizer.fit_transform(docs)\n",
    "    co_occurrence_matrix = (X.T * X)\n",
    "\n",
    "    # Compute word frequencies and total word count\n",
    "    word_counts = np.array(X.sum(axis=0)).squeeze()\n",
    "    total_word_count = word_counts.sum()\n",
    "\n",
    "    # Initialize PPMI matrix\n",
    "    ppmi_matrix = np.zeros(co_occurrence_matrix.shape)\n",
    "\n",
    "    # Compute PMI values\n",
    "    num_docs = len(docs)\n",
    "    for i, (word1, idx1) in enumerate(vectorizer.vocabulary_.items()):\n",
    "        for j, (word2, idx2) in enumerate(vectorizer.vocabulary_.items()):\n",
    "            co_occurrences = co_occurrence_matrix[idx1, idx2]\n",
    "            if co_occurrences == 0:\n",
    "                ppmi_matrix[idx1, idx2] = 0\n",
    "            else:\n",
    "                word1_count = word_counts[idx1]\n",
    "                word2_count = word_counts[idx2]\n",
    "                pmi = np.log((co_occurrences * num_docs) / (word1_count * word2_count))\n",
    "                ppmi = max(0, pmi)\n",
    "                ppmi_matrix[idx1, idx2] = ppmi\n",
    "\n",
    "    # Normalize PPMI matrix\n",
    "    ppmi_matrix = normalize(ppmi_matrix, norm='l2', axis=1)\n",
    "\n",
    "    return ppmi_matrix\n",
    "\n",
    "# Example usage:\n",
    "docs = [\n",
    "    \"This is a sample document containing some words.\",\n",
    "    \"Another document with different words but containing some similar words.\"\n",
    "]\n",
    "\n",
    "ppmi_matrix = ppmi_weighting(docs)\n",
    "print(ppmi_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d93055c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['Another', 'This', 'a', 'but', 'containing', 'different', 'document', 'is', 'sample', 'similar', 'some', 'with', 'words', 'words.']\n",
      "PPMI Matrix:\n",
      "[[0.         0.         0.         0.5        0.         0.5\n",
      "  0.         0.         0.         0.         0.         0.5\n",
      "  0.5        0.        ]\n",
      " [0.         0.         0.57735027 0.         0.         0.\n",
      "  0.         0.57735027 0.57735027 0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.57735027 0.         0.         0.         0.\n",
      "  0.         0.57735027 0.57735027 0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.4472136  0.         0.         0.         0.         0.4472136\n",
      "  0.         0.         0.         0.4472136  0.         0.4472136\n",
      "  0.4472136  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.4472136  0.         0.         0.4472136  0.         0.\n",
      "  0.         0.         0.         0.4472136  0.         0.4472136\n",
      "  0.4472136  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.57735027 0.57735027 0.         0.         0.\n",
      "  0.         0.         0.57735027 0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.57735027 0.57735027 0.         0.         0.\n",
      "  0.         0.57735027 0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.57735027 0.         0.57735027\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.57735027 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.5        0.         0.         0.5        0.         0.5\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.5        0.        ]\n",
      " [0.4472136  0.         0.         0.4472136  0.         0.4472136\n",
      "  0.         0.         0.         0.4472136  0.         0.4472136\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "def ppmi_weighting(docs, window_size=5):\n",
    "    # Tokenize the documents and build co-occurrence matrix\n",
    "    tokenized_docs = [doc.split() for doc in docs]\n",
    "    co_occurrence_matrix = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    for doc in tokenized_docs:\n",
    "        for i, word in enumerate(doc):\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(len(doc), i + window_size + 1)\n",
    "            context = doc[start:end]\n",
    "            for j in range(len(context)):\n",
    "                if context[j] != word:\n",
    "                    co_occurrence_matrix[word][context[j]] += 1\n",
    "\n",
    "    # Compute word frequencies and total word count\n",
    "    word_counts = defaultdict(int)\n",
    "    for doc in tokenized_docs:\n",
    "        for word in doc:\n",
    "            word_counts[word] += 1\n",
    "    total_word_count = sum(word_counts.values())\n",
    "\n",
    "    # Initialize PPMI matrix\n",
    "    ppmi_matrix = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "    # Compute PMI values\n",
    "    num_docs = len(docs)\n",
    "    for word, context in co_occurrence_matrix.items():\n",
    "        for co_word, co_count in context.items():\n",
    "            if co_count == 0:\n",
    "                ppmi_matrix[word][co_word] = 0\n",
    "            else:\n",
    "                pmi = np.log((co_count * num_docs) / (word_counts[word] * word_counts[co_word]))\n",
    "                ppmi = max(0, pmi)\n",
    "                ppmi_matrix[word][co_word] = ppmi\n",
    "\n",
    "    # Convert PPMI matrix to numpy array for easier handling\n",
    "    vocab = sorted(word_counts.keys())\n",
    "    ppmi_array = np.zeros((len(vocab), len(vocab)))\n",
    "    for i, word in enumerate(vocab):\n",
    "        for j, co_word in enumerate(vocab):\n",
    "            ppmi_array[i][j] = ppmi_matrix[word][co_word]\n",
    "\n",
    "    # Normalize PPMI matrix\n",
    "    ppmi_array = normalize(ppmi_array, norm='l2', axis=1)\n",
    "\n",
    "    return ppmi_array, vocab\n",
    "\n",
    "# Example usage:\n",
    "docs = [\n",
    "    \"This is a sample document containing some words.\",\n",
    "    \"Another document with different words but containing some similar words.\"\n",
    "]\n",
    "\n",
    "ppmi_matrix, vocab = ppmi_weighting(docs, window_size=5)\n",
    "print(\"Vocabulary:\", vocab)\n",
    "print(\"PPMI Matrix:\")\n",
    "print(ppmi_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a4130a",
   "metadata": {},
   "source": [
    "### Sublinear scaling involves taking the logarithm of the word counts or co-occurrence counts to dampen the effect of very frequent words. Let's modify the PPMI weighting function to incorporate sublinear scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c823057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['Another', 'This', 'a', 'but', 'containing', 'different', 'document', 'is', 'sample', 'similar', 'some', 'with', 'words', 'words.']\n",
      "PPMI Matrix:\n",
      "[[0.         0.         0.         0.49642982 0.         0.49642982\n",
      "  0.11928842 0.         0.         0.         0.         0.49642982\n",
      "  0.49642982 0.        ]\n",
      " [0.         0.         0.56654895 0.         0.13613753 0.\n",
      "  0.13613753 0.56654895 0.56654895 0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.55633203 0.         0.         0.13368248 0.\n",
      "  0.13368248 0.55633203 0.55633203 0.         0.13368248 0.\n",
      "  0.         0.13368248]\n",
      " [0.43722925 0.         0.         0.         0.10506296 0.43722925\n",
      "  0.10506296 0.         0.         0.43722925 0.10506296 0.43722925\n",
      "  0.43722925 0.10506296]\n",
      " [0.         0.21821789 0.21821789 0.21821789 0.         0.21821789\n",
      "  0.43643578 0.21821789 0.21821789 0.21821789 0.43643578 0.21821789\n",
      "  0.21821789 0.43643578]\n",
      " [0.43966253 0.         0.         0.43966253 0.10564766 0.\n",
      "  0.10564766 0.         0.         0.43966253 0.10564766 0.43966253\n",
      "  0.43966253 0.        ]\n",
      " [0.2773501  0.2773501  0.2773501  0.2773501  0.5547002  0.2773501\n",
      "  0.         0.2773501  0.2773501  0.         0.         0.2773501\n",
      "  0.2773501  0.        ]\n",
      " [0.         0.56137077 0.56137077 0.         0.13489325 0.\n",
      "  0.13489325 0.         0.56137077 0.         0.13489325 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.55633203 0.55633203 0.         0.13368248 0.\n",
      "  0.13368248 0.55633203 0.         0.         0.13368248 0.\n",
      "  0.         0.13368248]\n",
      " [0.         0.         0.         0.56137077 0.13489325 0.56137077\n",
      "  0.         0.         0.         0.         0.13489325 0.\n",
      "  0.56137077 0.13489325]\n",
      " [0.         0.         0.25       0.25       0.5        0.25\n",
      "  0.         0.25       0.25       0.25       0.         0.25\n",
      "  0.25       0.5       ]\n",
      " [0.48951305 0.         0.         0.48951305 0.11762637 0.48951305\n",
      "  0.11762637 0.         0.         0.         0.11762637 0.\n",
      "  0.48951305 0.        ]\n",
      " [0.43722925 0.         0.         0.43722925 0.10506296 0.43722925\n",
      "  0.10506296 0.         0.         0.43722925 0.10506296 0.43722925\n",
      "  0.         0.10506296]\n",
      " [0.         0.         0.2773501  0.2773501  0.5547002  0.\n",
      "  0.         0.         0.2773501  0.2773501  0.5547002  0.\n",
      "  0.2773501  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "def ppmi_weighting(docs, window_size=5):\n",
    "    # Tokenize the documents and build co-occurrence matrix\n",
    "    tokenized_docs = [doc.split() for doc in docs]\n",
    "    co_occurrence_matrix = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    for doc in tokenized_docs:\n",
    "        for i, word in enumerate(doc):\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(len(doc), i + window_size + 1)\n",
    "            context = doc[start:end]\n",
    "            for j in range(len(context)):\n",
    "                if context[j] != word:\n",
    "                    co_occurrence_matrix[word][context[j]] += 1\n",
    "\n",
    "    # Compute word frequencies and total word count\n",
    "    word_counts = defaultdict(int)\n",
    "    for doc in tokenized_docs:\n",
    "        for word in doc:\n",
    "            word_counts[word] += 1\n",
    "    total_word_count = sum(word_counts.values())\n",
    "\n",
    "    # Apply sublinear scaling to word counts\n",
    "    for word, count in word_counts.items():\n",
    "        word_counts[word] = 1 + np.log(count)  # Applying log function\n",
    "\n",
    "    # Initialize PPMI matrix\n",
    "    ppmi_matrix = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "    # Compute PMI values\n",
    "    num_docs = len(docs)\n",
    "    for word, context in co_occurrence_matrix.items():\n",
    "        for co_word, co_count in context.items():\n",
    "            if co_count == 0:\n",
    "                ppmi_matrix[word][co_word] = 0\n",
    "            else:\n",
    "                pmi = np.log((co_count * num_docs) / (word_counts[word] * word_counts[co_word]))\n",
    "                ppmi = max(0, pmi)\n",
    "                ppmi_matrix[word][co_word] = ppmi\n",
    "\n",
    "    # Convert PPMI matrix to numpy array for easier handling\n",
    "    vocab = sorted(word_counts.keys())\n",
    "    ppmi_array = np.zeros((len(vocab), len(vocab)))\n",
    "    for i, word in enumerate(vocab):\n",
    "        for j, co_word in enumerate(vocab):\n",
    "            ppmi_array[i][j] = ppmi_matrix[word][co_word]\n",
    "\n",
    "    # Normalize PPMI matrix\n",
    "    ppmi_array = normalize(ppmi_array, norm='l2', axis=1)\n",
    "\n",
    "    return ppmi_array, vocab\n",
    "\n",
    "# Example usage:\n",
    "docs = [\n",
    "    \"This is a sample document containing some words.\",\n",
    "    \"Another document with different words but containing some similar words.\"\n",
    "]\n",
    "\n",
    "ppmi_matrix, vocab = ppmi_weighting(docs, window_size=5)\n",
    "print(\"Vocabulary:\", vocab)\n",
    "print(\"PPMI Matrix:\")\n",
    "print(ppmi_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19613131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPMI Matrix:\n",
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.46885077 ... 0.46885077 0.         0.        ]\n",
      " [0.         0.39012075 0.         ... 0.39012075 0.39012075 0.        ]\n",
      " ...\n",
      " [0.         0.34158248 0.34158248 ... 0.         0.34158248 0.        ]\n",
      " [0.         0.         0.46885077 ... 0.46885077 0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "\n",
      "Synonyms with highest PPMI values for 'happy':\n",
      "['feel', 'when', 'weather', 'beautiful', 'but']\n",
      "\n",
      "Synonyms with highest PPMI values for 'sad':\n",
      "No synonyms found for 'sad'\n",
      "\n",
      "Synonyms with highest PPMI values for 'big':\n",
      "No synonyms found for 'big'\n",
      "\n",
      "Synonyms with highest PPMI values for 'small':\n",
      "No synonyms found for 'small'\n",
      "\n",
      "Synonyms with highest PPMI values for 'good':\n",
      "No synonyms found for 'good'\n",
      "\n",
      "Synonyms with highest PPMI values for 'bad':\n",
      "No synonyms found for 'bad'\n",
      "\n",
      "Synonyms with highest PPMI values for 'beautiful':\n",
      "['sunset', 'was', 'but', 'the', 'when']\n",
      "\n",
      "Synonyms with highest PPMI values for 'ugly':\n",
      "No synonyms found for 'ugly'\n",
      "\n",
      "Synonyms with highest PPMI values for 'fast':\n",
      "No synonyms found for 'fast'\n",
      "\n",
      "Synonyms with highest PPMI values for 'slow':\n",
      "No synonyms found for 'slow'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Function to get synonyms of a word using WordNet\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return list(synonyms)\n",
    "\n",
    "words = ['happy', 'sad', 'big', 'small', 'good', 'bad', 'beautiful', 'ugly', 'fast', 'slow']\n",
    "\n",
    "# Get synonyms for each word\n",
    "synonyms_dict = {}\n",
    "for word in words:\n",
    "    synonyms_dict[word] = get_synonyms(word)\n",
    "\n",
    "# Create a small corpus with sentences containing these words and their synonyms\n",
    "corpus = [\n",
    "    \"I feel {} when I'm {}.\".format(synonyms_dict['happy'][0], synonyms_dict['fast'][0]),\n",
    "    \"He looks {} when he's {}.\".format(synonyms_dict['sad'][0], synonyms_dict['slow'][0]),\n",
    "    \"The {} dog chased the {} cat.\".format(synonyms_dict['big'][0], synonyms_dict['small'][0]),\n",
    "    \"She said it was {} but it turned out to be {}.\".format(synonyms_dict['good'][0], synonyms_dict['bad'][0]),\n",
    "    \"The sunset was {} but the weather was {}.\".format(synonyms_dict['beautiful'][0], synonyms_dict['ugly'][0])\n",
    "]\n",
    "\n",
    "def ppmi_weighting(corpus):\n",
    "    co_occurrence_matrix = np.zeros((len(vocab), len(vocab)))\n",
    "    word_index = {word: i for i, word in enumerate(vocab)}\n",
    "    total_word_count = 0\n",
    "    for sentence in corpus:\n",
    "        words = sentence.split()\n",
    "        for i, word in enumerate(words):\n",
    "            if word in vocab:\n",
    "                total_word_count += 1\n",
    "                start = max(0, i - window_size)\n",
    "                end = min(len(words), i + window_size + 1)\n",
    "                context = words[start:end]\n",
    "                for j in range(len(context)):\n",
    "                    if context[j] != word and context[j] in vocab:\n",
    "                        co_occurrence_matrix[word_index[word]][word_index[context[j]]] += 1\n",
    "\n",
    "    # Compute PPMI values\n",
    "    ppmi_matrix = np.zeros_like(co_occurrence_matrix, dtype=float)\n",
    "    for i in range(len(vocab)):\n",
    "        for j in range(len(vocab)):\n",
    "            if co_occurrence_matrix[i][j] == 0:\n",
    "                ppmi_matrix[i][j] = 0\n",
    "            else:\n",
    "                ppmi_matrix[i][j] = max(0, np.log((co_occurrence_matrix[i][j] * total_word_count) /\n",
    "                                                  (word_counts[i] * word_counts[j])))\n",
    "\n",
    "    # Normalize PPMI matrix\n",
    "    ppmi_matrix = normalize(ppmi_matrix, norm='l2', axis=1)\n",
    "\n",
    "    return ppmi_matrix\n",
    "\n",
    "\n",
    "# Compute PPMI weighting for the corpus\n",
    "window_size = 2\n",
    "vectorizer = CountVectorizer(tokenizer=lambda text: text.split(), binary=True)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "word_counts = np.array(X.sum(axis=0)).squeeze()\n",
    "vocab = vectorizer.get_feature_names()\n",
    "ppmi_matrix = ppmi_weighting(corpus)\n",
    "\n",
    "# Print the PPMI matrix\n",
    "print(\"PPMI Matrix:\")\n",
    "print(ppmi_matrix)\n",
    "\n",
    "# Find the index of each word in the vocabulary\n",
    "word_indices = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "# Find synonyms with highest PPMI values for each word\n",
    "for word in words:\n",
    "    print(f\"\\nSynonyms with highest PPMI values for '{word}':\")\n",
    "    if word in word_indices:\n",
    "        word_index = word_indices[word]\n",
    "        ppmi_scores = ppmi_matrix[word_index]\n",
    "        top_synonyms_indices = ppmi_scores.argsort()[-5:][::-1]\n",
    "        top_synonyms = [vocab[idx] for idx in top_synonyms_indices if idx != word_index]\n",
    "        print(top_synonyms)\n",
    "    else:\n",
    "        print(f\"No synonyms found for '{word}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce30d413",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
