{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52cd36fa-6a58-4290-bbfb-e63834442ef2",
   "metadata": {},
   "source": [
    "## PyTorch modules and layers\n",
    "\n",
    "### What are modules?\n",
    "Module is an abstraction somewhere between neural network *layer* and complete *model*. In PyTorch, there all kinds of things one can do with modules and their combinations.\n",
    "\n",
    "![Multiple layers are combined into modules, forming repeating patterns of larger models.](./img/blocks.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3955d842-31f5-4165-99c6-1b0e7b57b74a",
   "metadata": {},
   "source": [
    "From a programming standpoint, a module is represented by a class. Any subclass of it must:\n",
    "  - define a forward propagation method that transforms its input into output\n",
    "  - store any necessary parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c79362-d815-476e-b74d-ec4807b4ec53",
   "metadata": {},
   "source": [
    "Start with necessary imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "009f256d-2d7f-4792-8dce-0207fea30075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7094c0d0-18eb-439b-8186-5a28c349ddde",
   "metadata": {},
   "source": [
    "### Module chaining\n",
    "We can use a built-in `Sequential` function to chain layers together. The below code creates a hidden layer with 256 neurons and ReLU activation function, and an output layer with 10 neurons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b94a2cc-6661-4bb9-868a-73493fd075c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))\n",
    "\n",
    "X = torch.rand(2, 20)\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c38b8d-e353-476a-9c99-3cd4264209a6",
   "metadata": {},
   "source": [
    "### Forward propagation\n",
    "**Important**: to use the model, we pass it the input data (`net(X)` in the above example). This executes the model's forward, along with some background operations.\n",
    "\n",
    "`net(X)` is actually just shorthand for `net.__call__(X)`. `LazyLinear` is a version of `Linear` that *infers* output dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef08a5c9-9085-4bc0-8d5e-4056ed3e54ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 20])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ad4a09-414d-4307-b5f8-1d9bfa6440b6",
   "metadata": {},
   "source": [
    "### Create a module from scratch\n",
    "In the following snippet, we code up a module from scratch corresponding to an MLP with one hidden layer with 256 hidden units, and a 10- dimensional output layer. Note that the MLP class below inherits the class that represents a module. We will heavily rely on the parent class’s methods, supplying only our own constructor (the __init__ method in Python) and the forward propagation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e824413e-ed71-4ed2-912d-5eda53d1ea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Call the constructor of the parent class nn.Module to perform\n",
    "        # the necessary initialization\n",
    "        super().__init__()\n",
    "        self.hidden = nn.LazyLinear(256)\n",
    "        self.out = nn.LazyLinear(10)\n",
    "\n",
    "    # Define the forward propagation of the model, that is, how to return the\n",
    "    # required model output based on the input X\n",
    "    def forward(self, X):\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8845d5d3-4d60-4331-9c80-289bf739274f",
   "metadata": {},
   "source": [
    "Let’s first focus on the forward propagation method. Note that it takes X as input, calculates the hidden representation with the activation function applied, and outputs its logits. In this MLP implementation, both layers are instance variables. To see why this is reasonable, imagine instantiating two MLPs, net1 and net2, and training them on different data. Naturally, we would expect them to represent two different learned models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb4a1e68-72bb-4d1c-9ff7-92ca972e7923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6d81a0-2fa9-46e3-8091-cf78b350785b",
   "metadata": {},
   "source": [
    "**So**: a module can abstract layers, complete models, or anything in-between."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf303a1-63ef-4c1d-bcdc-ebd8e3aa7306",
   "metadata": {},
   "source": [
    "### Writing your own `Sequential`\n",
    "\n",
    "We need to define two key methods:\n",
    "\n",
    "1. A method for appending modules one by one to a list.\n",
    "2. A forward propagation method for passing an input through the chain of modules, in the same order as they were appended.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36232efc-63b0-4d6a-a32d-30983e09f7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            self.add_module(str(idx), module)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for module in self.children():\n",
    "            X = module(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea71238b-f244-4ea8-8c4d-cca487f69fb1",
   "metadata": {},
   "source": [
    "Note `add_module` and `children` funcs.\n",
    "\n",
    "Now we can use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0984f53-1798-450b-97f6-9f053bfa7635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MySequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7594c962-1843-44a5-bb95-d288e09f1c73",
   "metadata": {},
   "source": [
    "### Executing Code in the Forward Propagation Method\n",
    "\n",
    "The `Sequential` class makes model construction easy,\n",
    "allowing us to assemble new architectures\n",
    "without having to define our own class.\n",
    "However, not all architectures are simple daisy chains.\n",
    "When greater flexibility is required,\n",
    "we will want to define our own blocks.\n",
    "For example, we might want to execute\n",
    "Python's control flow within the forward propagation method.\n",
    "Moreover, we might want to perform\n",
    "arbitrary mathematical operations,\n",
    "not simply relying on predefined neural network layers.\n",
    "\n",
    "You may have noticed that until now,\n",
    "all of the operations in our networks\n",
    "have acted upon our network's activations\n",
    "and its parameters.\n",
    "Sometimes, however, we might want to\n",
    "incorporate terms\n",
    "that are neither the result of previous layers\n",
    "nor updatable parameters.\n",
    "We call these *constant parameters*.\n",
    "Say for example that we want a layer\n",
    "that calculates the function\n",
    "$f(\\mathbf{x},\\mathbf{w}) = c \\cdot \\mathbf{w}^\\top \\mathbf{x}$,\n",
    "where $\\mathbf{x}$ is the input, $\\mathbf{w}$ is our parameter,\n",
    "and $c$ is some specified constant\n",
    "that is not updated during optimization.\n",
    "So we implement a `FixedHiddenMLP` class as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aee35bc6-88e6-4c40-9127-4c66611d441d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Random weight parameters that will not compute gradients and\n",
    "        # therefore keep constant during training\n",
    "        self.rand_weight = torch.rand((20, 20))\n",
    "        self.linear = nn.LazyLinear(20)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        X = F.relu(X @ self.rand_weight + 1)\n",
    "        # Reuse the fully connected layer. This is equivalent to sharing\n",
    "        # parameters with two fully connected layers\n",
    "        X = self.linear(X)\n",
    "        # Control flow\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3d7933-b614-4297-9f3f-38576e35d52a",
   "metadata": {},
   "source": [
    "In this model,\n",
    "we implement a hidden layer whose weights\n",
    "(`self.rand_weight`) are initialized randomly\n",
    "at instantiation and are thereafter constant.\n",
    "This weight is not a model parameter\n",
    "and thus it is never updated by backpropagation.\n",
    "The network then passes the output of this \"fixed\" layer\n",
    "through a fully connected layer.\n",
    "\n",
    "Note that before returning the output,\n",
    "our model did something unusual.\n",
    "We ran a while-loop, testing\n",
    "on the condition its $\\ell_1$ norm is larger than $1$,\n",
    "and dividing our output vector by $2$\n",
    "until it satisfied the condition.\n",
    "Finally, we returned the sum of the entries in `X`.\n",
    "To our knowledge, no standard neural network\n",
    "performs this operation.\n",
    "Note that this particular operation may not be useful\n",
    "in any real-world task.\n",
    "Our point is only to show you how to integrate\n",
    "arbitrary code into the flow of your\n",
    "neural network computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15c7a145-4a67-4c0d-a1c9-7f7a10783ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1262, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1718c62f-71a0-4178-b332-24baf27b8866",
   "metadata": {},
   "source": [
    "We can mix and match various ways of assembling modules together. In the following example, we nest modules in some creative ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec0b3fcb-bd1d-47ff-98ee-58ee6d852b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0111, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.LazyLinear(64), nn.ReLU(),\n",
    "                                 nn.LazyLinear(32), nn.ReLU())\n",
    "        self.linear = nn.LazyLinear(16)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera = nn.Sequential(NestMLP(), nn.LazyLinear(20), FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ed6645-5c20-431e-ad47-4f119fad59fa",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "Once we have chosen an architecture\n",
    "and set our hyperparameters,\n",
    "we proceed to the training loop,\n",
    "where our goal is to find parameter values\n",
    "that minimize our loss function.\n",
    "After training, we will need these parameters\n",
    "in order to make future predictions.\n",
    "Additionally, we will sometimes wish\n",
    "to extract the parameters\n",
    "perhaps to reuse them in some other context,\n",
    "to save our model to disk so that\n",
    "it may be executed in other software,\n",
    "or for examination in the hope of\n",
    "gaining scientific understanding.\n",
    "\n",
    "Most of the time, we will be able\n",
    "to ignore the nitty-gritty details\n",
    "of how parameters are declared\n",
    "and manipulated, relying on deep learning frameworks\n",
    "to do the heavy lifting.\n",
    "However, when we move away from\n",
    "stacked architectures with standard layers,\n",
    "we will sometimes need to get into the weeds\n",
    "of declaring and manipulating parameters.\n",
    "In this section, we cover the following:\n",
    "\n",
    "* Accessing parameters for debugging, diagnostics, and visualizations.\n",
    "* Sharing parameters across different model components.\n",
    "\n",
    "Let's use an MLP with one hidden layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fe8a590-2758-4ccf-a9bd-3308117b35a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.LazyLinear(8),\n",
    "                    nn.ReLU(),\n",
    "                    nn.LazyLinear(1))\n",
    "\n",
    "X = torch.rand(size=(2, 4))\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d166a42-7117-470e-9324-fcde37d10ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6713, 0.6444, 0.2573, 0.3540],\n",
       "        [0.8819, 0.8942, 0.4940, 0.5255]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b626984-f02f-408d-9ff4-63cde5d51ed6",
   "metadata": {},
   "source": [
    "How do we access each layer's parameters? Via indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7208f765-47b0-4db4-b062-35e09ab42b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[ 0.2974, -0.0467,  0.1068, -0.3118],\n",
       "                      [ 0.0492, -0.0361,  0.1850,  0.1619],\n",
       "                      [-0.3017,  0.4390,  0.3953,  0.4186],\n",
       "                      [-0.0141,  0.4589, -0.4912,  0.0340],\n",
       "                      [-0.1112,  0.1139,  0.3528,  0.4048],\n",
       "                      [-0.2738,  0.4053, -0.3253,  0.4379],\n",
       "                      [ 0.3692,  0.0327, -0.4255, -0.2962],\n",
       "                      [ 0.4059, -0.1000,  0.3739,  0.1108]])),\n",
       "             ('bias',\n",
       "              tensor([ 0.2064,  0.2582, -0.1349,  0.1333, -0.0423,  0.1647,  0.3899,  0.3544]))])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18b76f87-fc49-4d6c-adab-e1229742cd2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[-0.1288, -0.2942, -0.0884, -0.0006,  0.1810,  0.2459,  0.1856,  0.2715]])),\n",
       "             ('bias', tensor([0.0476]))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfc73fb-d2bb-4790-bf88-0610ce02bd68",
   "metadata": {},
   "source": [
    "Note that parameters (weights/bias) are instances of a `torch.nn.parameter.Parameter` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ce71956-0d70-4d66-b2aa-d646a57a88cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1288, -0.2942, -0.0884, -0.0006,  0.1810,  0.2459,  0.1856,  0.2715]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129bbfaf-cf3b-4fdb-836d-89999526a02e",
   "metadata": {},
   "source": [
    "We can use `data` field in order to extract actual data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49b5c519-9cbd-489b-9b5e-1058d1bbe075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1288, -0.2942, -0.0884, -0.0006,  0.1810,  0.2459,  0.1856,  0.2715]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a283ef3f-045b-4043-8154-d975e0150947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd1d3012-1556-4fe6-9c25-dba619b0062c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a46d7b-ae83-485a-acd3-47865e5db528",
   "metadata": {},
   "source": [
    "We can also access gradients for each layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92748a77-d83c-4333-982e-88782db78f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].weight.grad == None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7586896-42c5-433b-bcdc-fb2d78bb2965",
   "metadata": {},
   "source": [
    "We can also access parameters all at once via `nn.Module.named_parameters()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "280ff598-6c7a-4c0f-9991-253e6f62c551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0.weight', torch.Size([8, 4])),\n",
       " ('0.bias', torch.Size([8])),\n",
       " ('2.weight', torch.Size([1, 8])),\n",
       " ('2.bias', torch.Size([1]))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(name, param.shape) for name, param in net.named_parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb65aae-bfb7-475e-81e5-55029fe3a9b4",
   "metadata": {},
   "source": [
    "We can also tie or connect/share parameters in different layers.\n",
    "\n",
    "In the following we allocate a fully connected layer\n",
    "and then use its parameters specifically\n",
    "to set those of another layer.\n",
    "Here we need to run the forward propagation\n",
    "`net(X)` before accessing the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef26c0f2-a5a5-4757-afa6-5528ea52030d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "# We need to give the shared layer a name so that we can refer to its\n",
    "# parameters\n",
    "shared = nn.LazyLinear(8)\n",
    "net = nn.Sequential(nn.LazyLinear(8), nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    nn.LazyLinear(1))\n",
    "\n",
    "net(X)\n",
    "# Check whether the parameters are the same\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
    "net[2].weight.data[0, 0] = 100\n",
    "# Make sure that they are actually the same object rather than just having the\n",
    "# same value\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c9c4a0-c91e-4ca5-847c-22a757bf6381",
   "metadata": {},
   "source": [
    "This example shows that the parameters\n",
    "of the second and third layer are tied.\n",
    "They are not just equal, they are\n",
    "represented by the same exact tensor.\n",
    "Thus, if we change one of the parameters,\n",
    "the other one changes, too.\n",
    "\n",
    "You might wonder,\n",
    "when parameters are tied\n",
    "what happens to the gradients?\n",
    "Since the model parameters contain gradients,\n",
    "the gradients of the second hidden layer\n",
    "and the third hidden layer are added together\n",
    "during backpropagation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8f9c29-3e21-4fff-9d4b-82713730eb59",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "\n",
    "By default, PyTorch initializes weight and bias matrices uniformly by drawing from a range that is computed according to the input and output dimension. PyTorch’s nn.init module provides a variety of preset initialization methods.\n",
    "\n",
    "Let's start with a sample neural network built with `nn.Sequential`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be23bc30-95af-46fc-99b3-450030659984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.LazyLinear(8), nn.ReLU(), nn.LazyLinear(1))\n",
    "X = torch.rand(size=(2, 4))\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de35b2f8-8287-432e-a656-817e36062f2b",
   "metadata": {},
   "source": [
    "### Built-in initializers\n",
    "\n",
    "This code will initialize weights as Guassian random variables, and biases will be 0. The `apply` method applies the given function recursively to each submodule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d078f42-0fd6-482c-808f-afea9c995109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0027, -0.0078,  0.0168, -0.0066]), tensor(0.))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_normal(module):\n",
    "    if type(module) == nn.Linear:\n",
    "        nn.init.normal_(module.weight, mean=0, std=0.01)\n",
    "        nn.init.zeros_(module.bias)\n",
    "\n",
    "net.apply(init_normal)\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb362fac-dbda-4ab3-bae5-058d7599908e",
   "metadata": {},
   "source": [
    "Or we can use constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37a98316-d0ee-4596-95da-a25989dec4e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1.]), tensor(0.))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_constant(module):\n",
    "    if type(module) == nn.Linear:\n",
    "        nn.init.constant_(module.weight, 1)\n",
    "        nn.init.zeros_(module.bias)\n",
    "\n",
    "net.apply(init_constant)\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e804dd21-9c49-4195-bca1-71f779d00b02",
   "metadata": {},
   "source": [
    "We can use indexing to initialize parameters for each layer separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd6aa9b3-e087-48cb-baae-08c11dbe2fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.2783, -0.5206,  0.5574, -0.3472])\n",
      "tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n"
     ]
    }
   ],
   "source": [
    "def init_xavier(module):\n",
    "    if type(module) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(module.weight)\n",
    "\n",
    "def init_42(module):\n",
    "    if type(module) == nn.Linear:\n",
    "        nn.init.constant_(module.weight, 42)\n",
    "\n",
    "net[0].apply(init_xavier)\n",
    "net[2].apply(init_42)\n",
    "print(net[0].weight.data[0])\n",
    "print(net[2].weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b55dd2-67e4-4551-abc2-aaeb83429bbc",
   "metadata": {},
   "source": [
    "Sometimes, the initialization methods we need\n",
    "are not provided by the deep learning framework.\n",
    "In the example below, we define an initializer\n",
    "for any weight parameter $w$ using the following strange distribution (here $U$ stands for uniform distribution):\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    w \\sim \\begin{cases}\n",
    "        U(5, 10) & \\textrm{ with probability } \\frac{1}{4} \\\\\n",
    "            0    & \\textrm{ with probability } \\frac{1}{2} \\\\\n",
    "        U(-10, -5) & \\textrm{ with probability } \\frac{1}{4}\n",
    "    \\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24cd78f1-0179-4cca-a203-18b9d987030b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init weight torch.Size([8, 4])\n",
      "Init weight torch.Size([1, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.4771, -7.0744, -0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000, -9.7832, -0.0000]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_init(module):\n",
    "    if type(module) == nn.Linear:\n",
    "        print(\"Init\", *[(name, param.shape)\n",
    "                        for name, param in module.named_parameters()][0])\n",
    "        nn.init.uniform_(module.weight, -10, 10)\n",
    "        module.weight.data *= module.weight.data.abs() >= 5\n",
    "\n",
    "net.apply(my_init)\n",
    "net[0].weight[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68eefa29-b8fd-4aeb-a259-8f87cac43d1f",
   "metadata": {},
   "source": [
    "And, of course, we can always set the parameters directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02fee178-4573-4422-aceb-863763988b94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42.0000, -6.0744,  1.0000,  1.0000])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data[:] += 1\n",
    "net[0].weight.data[0, 0] = 42\n",
    "net[0].weight.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe3f9b2-0ec4-4555-827a-1506b7c37097",
   "metadata": {},
   "source": [
    "Or in model class constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7aeb3be9-5835-4870-8df1-2def58908c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.weights = nn.Parameter(torch.randn(1, # <- start with random weights (this will get adjusted as the model learns)\n",
    "                                                dtype=torch.float), # <- PyTorch loves float32 by default\n",
    "                                   requires_grad=True) # <- can we update this value with gradient descent?)\n",
    "\n",
    "        self.bias = nn.Parameter(torch.randn(1, # <- start with random bias (this will get adjusted as the model learns)\n",
    "                                            dtype=torch.float), # <- PyTorch loves float32 by default\n",
    "                                requires_grad=True) # <- can we update this value with gradient descent?))\n",
    "\n",
    "    # Forward defines the computation in the model\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor: # <- \"x\" is the input data (e.g. training/testing features)\n",
    "        return self.weights * x + self.bias # <- this is the linear regression formula (y = m*x + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769aa889-d479-4051-8795-a3ad95511b72",
   "metadata": {},
   "source": [
    "### Lazy initialization\n",
    "\n",
    "PyTorch can defer parameter initialization, waiting until the first time we pass data through the model, to infer the sizes of each layer on the fly.\n",
    "\n",
    "Later on, when working with convolutional neural networks, this technique will become even more convenient since the input dimensionality (e.g., the resolution of an image) will affect the dimensionality of each subsequent layer.\n",
    "\n",
    "Let's create some perceptron again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea1b347b-a7e5-4b6f-af39-9d522904f27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6ab599-6d9e-458e-9572-0c49362e606f",
   "metadata": {},
   "source": [
    "Check first layer params:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a448bcc7-5759-433d-a365-cb209934583e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<UninitializedParameter>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b5d88e-fad4-42e7-8a4e-c89d0b736e62",
   "metadata": {},
   "source": [
    "We can pass some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d42255e9-35f3-4e51-95d0-42a53d1c10f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 20])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(2, 20)\n",
    "net(X)\n",
    "\n",
    "net[0].weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6da710-892e-470b-88fc-42e96f5a8a3a",
   "metadata": {},
   "source": [
    "### Custom layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2931d155-3ead-4870-afe7-4c34e566ca43",
   "metadata": {},
   "source": [
    "Let's first try a layer without parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "54fb24d6-05dc-4d9b-884f-80cd6988f0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenteredLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, X):\n",
    "        return X - X.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c74142-9ebe-42b9-973a-1f3826f32987",
   "metadata": {},
   "source": [
    "Let's see if it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "95c96fe2-d4b3-47d3-aff1-25dbb35df111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2., -1.,  0.,  1.,  2.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = CenteredLayer()\n",
    "layer(torch.tensor([1.0, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2890d991-407c-4e69-b58f-2487606330ff",
   "metadata": {},
   "source": [
    "We can use it in some models now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b1a5704-127c-4cc5-ae4c-c6801c7fc8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.LazyLinear(128), CenteredLayer())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58e5a25-30ce-41d8-b099-49758ddc829c",
   "metadata": {},
   "source": [
    "Check how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "318398b4-6dec-4b34-addc-cb3d662e032d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-5.3551e-09, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = net(torch.rand(4, 8))\n",
    "Y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5b29d8-e64d-465d-9dcc-2203d562b5da",
   "metadata": {},
   "source": [
    "The very small number instead of 0 is due to floating point number arithmetic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5441bd-1d00-433c-a752-86a239d9decb",
   "metadata": {},
   "source": [
    "Let's now create a custom layer with parameters: `in_units` and `out_units` for inputs and outputs counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2955e5f8-ceac-4371-b204-33d9047005de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, in_units, out_units):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(in_units, out_units))\n",
    "        self.bias = nn.Parameter(torch.randn(out_units,))\n",
    "\n",
    "    def forward(self, X):\n",
    "        linear = torch.matmul(X, self.weight.data) + self.bias.data\n",
    "        return F.relu(linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b240c690-1be7-4939-a39a-49fd31057b76",
   "metadata": {},
   "source": [
    "We can instantiate it and check parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5a2ae7a4-af2b-40b6-8673-901c68189221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1364,  1.0427, -1.5300],\n",
       "        [-0.3602, -1.2868, -0.1001],\n",
       "        [ 0.6256,  0.7194, -2.0411],\n",
       "        [ 0.4784, -0.5905,  0.6690],\n",
       "        [ 0.6394, -0.2392, -0.0049]], requires_grad=True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = MyLinear(5, 3)\n",
    "linear.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acaa9171-eb11-498e-b185-ef37191fc93d",
   "metadata": {},
   "source": [
    "Now invoke forward propagation using our custom layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "00f22d25-47ab-47b7-bec7-290a985b2120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.8444, 0.0000, 0.0000],\n",
       "        [1.5298, 0.2254, 0.0000]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear(torch.rand(2, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fa6e5a-f5a1-407b-b0c3-3f53277b2859",
   "metadata": {},
   "source": [
    "Now we can construct a model using the custom layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9fa22724-744d-402e-b5fb-964c62863789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12.7743],\n",
       "        [14.6024]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(MyLinear(64, 8), MyLinear(8, 1))\n",
    "net(torch.rand(2, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f2b8b0-5cca-4247-90fb-b4da40614cd5",
   "metadata": {},
   "source": [
    "### Loading and saving\n",
    "\n",
    "Save a single tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "40756f8d-ceba-4f85-aa06-b34d54a8e3ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4)\n",
    "torch.save(x, 'torch.data')\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1bdbde-7de6-4254-bd3e-edd59b332b66",
   "metadata": {},
   "source": [
    "Now read it back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ceb30ed0-c269-4841-b38b-35f67dbc9ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = torch.load('torch.data')\n",
    "x2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434bce76-513c-41cd-9bbd-0de789817f09",
   "metadata": {},
   "source": [
    "Same can be done with lists of tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aac3fe5c-fea1-43a4-b89c-999b413b3425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3]), tensor([0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.zeros(4)\n",
    "torch.save([x, y],'torch2.data')\n",
    "x2, y2 = torch.load('torch2.data')\n",
    "(x2, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8175d2-d2f3-4604-9e03-c7a9beb04ef8",
   "metadata": {},
   "source": [
    "Saving dictionaries of tensors also works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "af7adfb6-5175-41d3-a5e4-27afb905a139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([0, 1, 2, 3]), 'y': tensor([0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydict = {'x': x, 'y': y}\n",
    "torch.save(mydict, 'torch_dict.data')\n",
    "mydict2 = torch.load('torch_dict.data')\n",
    "mydict2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c72a14-beb9-4f73-94da-c7427d5b5e5d",
   "metadata": {},
   "source": [
    "How does this work for complete models? Let's check it, using the example model for multi-layer perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9e7fe20f-c33d-4506-93d3-72104c73a2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.LazyLinear(256)\n",
    "        self.output = nn.LazyLinear(10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.output(F.relu(self.hidden(x)))\n",
    "\n",
    "net = MLP()\n",
    "X = torch.randn(size=(2, 20))\n",
    "Y = net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75347af6-8ffb-4020-b46f-b9a657267d47",
   "metadata": {},
   "source": [
    "We can save the parameters of that model with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "28e4f792-56f6-4d25-952a-9576322f37f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'mlp.params')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce711cb-a97d-4f48-90b8-697761281a1a",
   "metadata": {},
   "source": [
    "In order to load the model, we can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "69336b4e-99e7-438c-94f2-b4d7e03eb67a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (hidden): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "  (output): LazyLinear(in_features=0, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clone = MLP()\n",
    "clone.load_state_dict(torch.load('mlp.params'))\n",
    "clone.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbfa7f5-830c-4258-b700-a73f4e5d62db",
   "metadata": {},
   "source": [
    "Let's verify that original model and cloned model produce same results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9cf82b70-e5e0-41bd-a2b3-0773953c2188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_clone = clone(X)\n",
    "Y_clone == Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099bea2b-71f0-4800-9781-44c8e26c6dfb",
   "metadata": {},
   "source": [
    "## Walkthrough with TorchVision dataset\n",
    "\n",
    "### Fetching data\n",
    "Let's fetch some sample dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a4b5e687-2000-4ce8-bf0d-4f44f4dbc2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9588f7-92ee-459a-bf00-aec8547ecb22",
   "metadata": {},
   "source": [
    "PyTorch offers domain-specific libraries such as TorchText, TorchVision, and TorchAudio, all of which include datasets. For this tutorial, we will be using a TorchVision dataset.\n",
    "\n",
    "The torchvision.datasets module contains Dataset objects for many real-world vision data like CIFAR, COCO (full list here). We will use the FashionMNIST dataset. Every TorchVision Dataset includes two arguments: transform and target_transform to modify the samples and labels respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "83967f42-c8d9-41b3-88c6-49b872a0989b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 26421880/26421880 [00:26<00:00, 987520.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 29515/29515 [00:00<00:00, 545342.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 4422102/4422102 [00:01<00:00, 2288719.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 5148/5148 [00:00<00:00, 5148373.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d4ba6f-e072-4f65-93e3-deb0f888e3c7",
   "metadata": {},
   "source": [
    "We pass the `Dataset` as an argument to `DataLoader`. This wraps an iterable over our dataset, and supports automatic batching, sampling, shuffling and multiprocess data loading. Here we define a batch size of 64, i.e. each element in the dataloader iterable will return a batch of 64 features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "597c3fef-7209-4c6c-8bb6-557c63955fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8861a472-71dd-4bbe-9b0c-d2df1905f1d8",
   "metadata": {},
   "source": [
    "Let's visualize some samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3df3b417-8851-4ea9-836b-507f369d1a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732a2043-c7b2-434a-891f-971bb22fe03d",
   "metadata": {},
   "source": [
    "### Transforms\n",
    "\n",
    "All TorchVision datasets have two parameters -transform to modify the features and target_transform to modify the labels - that accept callables containing the transformation logic. The torchvision.transforms module offers several commonly-used transforms out of the box.\n",
    "\n",
    "The FashionMNIST features are in PIL Image format, and the labels are integers. For training, we need the features as normalized tensors, and the labels as one-hot encoded tensors. To make these transformations, we can use ToTensor and Lambda.\n",
    "\n",
    "`target_transform` defines a function to turn the integer into a one-hot encoded tensor. It first creates a zero tensor of size 10 (the number of labels in our dataset) and calls scatter_ which assigns a value=1 on the index as given by the label y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a1fb1bea-10b1-4d85-8da1-faca418f2a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "ds = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05915445-9ffc-44f8-a14a-ce0d94bf9497",
   "metadata": {},
   "source": [
    "### Defining a network model\n",
    "\n",
    "Now that we have the test data, define a neural network with 2 hidden layers and ReLU activation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ab4240e5-8e19-48b1-ac42-1dbf6d7c745e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ecf728-4384-4e2a-988f-3f248be6a03c",
   "metadata": {},
   "source": [
    "Let's break down the steps:\n",
    "  - initialize the `nn.Flatten` layer to convert each 2D 28x28 image into a contiguous array of 784 pixel values ( the minibatch dimension (at dim=0) is maintained).\n",
    "  -  `nn.Linear` is a module that applies a linear transformation on the input using its stored weights and biases.\n",
    "  -  `nn.ReLU` activations are applied after linear transformations to introduce nonlinearity.\n",
    "  -  `nn.Sequential` orders the modules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d12a6b-0fc2-4c82-b810-5e35e7baddda",
   "metadata": {},
   "source": [
    "In order to train a model, we also need to specify a loss function and optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c3647e12-fb37-41db-86f0-3b1be736ad9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4762ff64-e440-4c79-8f85-b98d2c69da3c",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "To sum up, in order to train a model, we need the following:\n",
    "  - the dataloader that wraps over some dataset\n",
    "  - model itself (derived from PyTorch's `nn.Module`)\n",
    "  - loss function\n",
    "  - optimizer\n",
    "\n",
    "Needed steps are:\n",
    "  - compure forward propagation\n",
    "  - calculate loss\n",
    "  - compute backward propagation\n",
    "  - perform GD step\n",
    "\n",
    "![A picture](./img/training_loop.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7ec9e6a9-13dc-4219-8e88-97ed4b7085be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # This just sets the model in train mode\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa91cc09-dd22-49bd-aff6-033b0fdac101",
   "metadata": {},
   "source": [
    "We'll also need a test function in order to verify how our model works. Note that we use `torch.no_grad()` as we don't need the gradients when testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "055b54f0-d711-48b3-8e61-4dfbfff167d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ad2377-11bf-4ad2-a381-9daee9d92436",
   "metadata": {},
   "source": [
    "![A picture](./img/testing_loop.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8becd2-820e-44fe-a89d-993faf5fd7c8",
   "metadata": {},
   "source": [
    "Let's run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "313d8ead-58f6-4aad-81d5-079e1aff19f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.308726  [    0/60000]\n",
      "loss: 2.289409  [ 6400/60000]\n",
      "loss: 2.270619  [12800/60000]\n",
      "loss: 2.268641  [19200/60000]\n",
      "loss: 2.257281  [25600/60000]\n",
      "loss: 2.223632  [32000/60000]\n",
      "loss: 2.231599  [38400/60000]\n",
      "loss: 2.194693  [44800/60000]\n",
      "loss: 2.199749  [51200/60000]\n",
      "loss: 2.168252  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 55.1%, Avg loss: 2.161203 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.170470  [    0/60000]\n",
      "loss: 2.159441  [ 6400/60000]\n",
      "loss: 2.103426  [12800/60000]\n",
      "loss: 2.131146  [19200/60000]\n",
      "loss: 2.081449  [25600/60000]\n",
      "loss: 2.024529  [32000/60000]\n",
      "loss: 2.047542  [38400/60000]\n",
      "loss: 1.962532  [44800/60000]\n",
      "loss: 1.974407  [51200/60000]\n",
      "loss: 1.921622  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.1%, Avg loss: 1.904929 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.927534  [    0/60000]\n",
      "loss: 1.904148  [ 6400/60000]\n",
      "loss: 1.782693  [12800/60000]\n",
      "loss: 1.841961  [19200/60000]\n",
      "loss: 1.735695  [25600/60000]\n",
      "loss: 1.677568  [32000/60000]\n",
      "loss: 1.700515  [38400/60000]\n",
      "loss: 1.582150  [44800/60000]\n",
      "loss: 1.613892  [51200/60000]\n",
      "loss: 1.523800  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.7%, Avg loss: 1.528768 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.584466  [    0/60000]\n",
      "loss: 1.554964  [ 6400/60000]\n",
      "loss: 1.395467  [12800/60000]\n",
      "loss: 1.490286  [19200/60000]\n",
      "loss: 1.367846  [25600/60000]\n",
      "loss: 1.346471  [32000/60000]\n",
      "loss: 1.362689  [38400/60000]\n",
      "loss: 1.267759  [44800/60000]\n",
      "loss: 1.318516  [51200/60000]\n",
      "loss: 1.216540  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.0%, Avg loss: 1.246144 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.318137  [    0/60000]\n",
      "loss: 1.301617  [ 6400/60000]\n",
      "loss: 1.131638  [12800/60000]\n",
      "loss: 1.255630  [19200/60000]\n",
      "loss: 1.131414  [25600/60000]\n",
      "loss: 1.138344  [32000/60000]\n",
      "loss: 1.160050  [38400/60000]\n",
      "loss: 1.081013  [44800/60000]\n",
      "loss: 1.139199  [51200/60000]\n",
      "loss: 1.049110  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 1.077139 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93478094-c439-40f2-8bdb-f6ba8d19d04c",
   "metadata": {},
   "source": [
    "Save and load model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b8970de6-ed4a-4c0d-8280-de113c03743a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "model = NeuralNetwork()\n",
    "model.load_state_dict(torch.load(\"model.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2d58eb-fb28-4dac-aba9-61ba4d123bec",
   "metadata": {},
   "source": [
    "Let's use the model to make predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "718577ea-e1ed-4831-bc21-581f0dadfad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "# Switch model to evaluation mode\n",
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5b2119-0ba9-49d6-84e0-b9b21395fa8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
